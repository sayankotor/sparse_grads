{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146bcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, BertConfig, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter,   UninitializedParameter\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad28d5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_collecting_tensors(step, tensor1, tensor2=None):\n",
    "    '''собирает в тензор     '''\n",
    "    if step == 0:\n",
    "        return tensor1.unsqueeze(0)\n",
    "    else:\n",
    "        return torch.concatenate((tensor1, tensor2),0)\n",
    "    \n",
    "\n",
    "\n",
    "def Tucker_Decomposition(tensor):\n",
    "    n1, n2, n3 = tensor.shape\n",
    "    u1, _, _ = torch.svd(torch.reshape(tensor, (n1, -1)))\n",
    "    u2, _, _ = torch.svd(torch.reshape(torch.permute(tensor, [1, 2, 0]), (n2, -1)))\n",
    "    u3, _, _ = torch.svd(torch.reshape(torch.permute(tensor, [2, 0, 1]), (n3, -1)))\n",
    "    return u1, u2, u3\n",
    "\n",
    "\n",
    "def get_tucker_tensors(dict_layers):\n",
    "    '''делает словарь где ключом будет слой, а значением будет тензор'''        \n",
    "    dict_tensor = dict(zip(range(12), [[]]*12))\n",
    "    for key in dict_layers.keys():\n",
    "        dict_tensor[key].append(torch.cat(dict_layers[key], 2 ))\n",
    "    return dict_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b562d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearFunction(torch.autograd.Function):\n",
    "\n",
    "        # Note that forward, setseup_context, and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    def forward(ctx,input, weight, bias, grads, treshold):\n",
    "        treshold = treshold\n",
    "        if ((len(grads) <30 )):\n",
    "\n",
    "            ctx.save_for_backward(input, weight, bias)\n",
    "        else:\n",
    "            u1, U, VT =Tucker_Decomposition(torch.cat(MyLayer.grads))\n",
    "            ctx.save_for_backward(input,weight, bias, U, VT)\n",
    "        ctx.size = input.shape[0]\n",
    "\n",
    "        return  input @ weight.T  + bias\n",
    "    \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        if len(ctx.saved_tensors) == 3:\n",
    "            input,weight, bias = ctx.saved_tensors\n",
    "            grad_input = grad_weight = grad_bias = None\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                grad_input = grad_output @ weight\n",
    "            if ctx.needs_input_grad[1]:\n",
    "                grad_weight =  grad_output.T @input\n",
    "  \n",
    "            if bias is not None and ctx.needs_input_grad[2]:\n",
    "                grad_bias = grad_output\n",
    "            \n",
    "        elif len(ctx.saved_tensors) == 5:\n",
    "\n",
    "            input,weight, bias, U, VT = ctx.saved_tensors\n",
    "            grad_input = grad_weight = grad_bias = None\n",
    "            if ctx.needs_input_grad[0]:\n",
    "                grad_input = grad_output @ weight\n",
    "            if ctx.needs_input_grad[1]:\n",
    "                grad_weight = grad_output.T @input \n",
    "                grad_weight = U @  grad_weight @ VT\n",
    "                grad_weight = torch.where(torch.abs(grad_weight) >= treshold, grad_weight, 0)\n",
    "            if bias is not None and ctx.needs_input_grad[2]:\n",
    "                grad_bias = grad_output\n",
    "            \n",
    "        return grad_input, grad_weight, grad_bias, None, None\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6b81bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(torch.nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.grads = []\n",
    "        self.treshold = 1e-3\n",
    "        \n",
    "        \n",
    "    def from_pretrained(self, nn.Linear):\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "            return MyLinearFunction.apply(x, self.weight, self.bias, self.grads, self.treshold)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbe0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= torch.ones((1,10))\n",
    "a = torch.nn.Parameter(torch.randn((10,10)))\n",
    "b = torch.nn.Parameter(torch.randn((10,10)))\n",
    "c = torch.nn.Parameter(torch.randn((1, 10)))\n",
    "u = torch.randn((10,10)) \n",
    "vt = torch.randn((10,10))\n",
    "treshold = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1514c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MyLayer = MyLinear(10, 10)\n",
    "MyLayer.weight = nn.Parameter(a, requires_grad = True)\n",
    "MyLayer.bias = nn.Parameter(c, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fcef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = torch.nn.Parameter(torch.randn((1, 10)))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(MyLayer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a313a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    optimizer.zero_grad()\n",
    "    output = MyLayer(x)\n",
    "    loss = criterion(output, gt)\n",
    "  \n",
    "    loss.backward()\n",
    "    MyLayer.grads.append(MyLayer.weight.grad.unsqueeze(0))\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a2c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = MyLayer(x)\n",
    "loss = criterion(output, gt)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b572de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
